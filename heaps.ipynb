{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priority Queues (Heaps)\n",
    "**Priority Queues** are a special kind of queue in which instead of first in, first out we create a function that specifies the order of which item is at the front. For example, a printer that uses a queue to process which jobs to run. With a regular queue we run jobs in the order they arrived, however lets say we have several 1-page jobs and one 100-page job, it might be reasonable to run the 1-page jobs first and the 100-page job last. With a priority queue we can supply some order of precedence that the 1-page jobs run first, while maintaining the order in which they arrived, and the 100-page job runs last. \n",
    "\n",
    "Unlike a regular queue that had the operations enqueue and dequeue, priority queues use insert and deleteMin. You can think of insert as enqueue and deleteMin as dequeue. \n",
    "    \n",
    "## Implementations Using Previous Data Structures\n",
    "There are several ways we could implement a priority queue. Similar to a regular queue we could use a linked list and perform insertions at the front, O(1), and then traverse the list, O(N), to delete the minimun element. Alternatively we could always keep the list sorted but this reverses the run time in that insertions now take linear time (having to resort the list each insertion) and deletes constant time. \n",
    "\n",
    "Another way of implementing priority queues would be to use binary search trees which have a O(logN) running time for both insertion and deletion. The problem is we are removing the minimum element everytime so the left subtree would be constantly imbalanced. So in a AVL or Red-Black tree the number of rebalances would be higher than the average tree. \n",
    "\n",
    "## Binary Heap\n",
    "Instead of using linked list or binary search trees we will use a structure called a **binary heap**. Throughout the lecture we will refer to binary heaps simply as heaps. A heap is a binary search tree that is completely filled, with the exception of bottom level. Meaning that each node, besides the leaves, always have two children. We also fill the tree from left to right. Such a tree is known as a **complete binary tree** \n",
    "\n",
    "<img src=\"./files/Heaps/complete-binary-tree.png\" width=\"300\"/>\n",
    "\n",
    "As we learned earlier in the trees lecture a binary search tree can be implemented as well using a list to maintain the items. Such that for any position i, the left child is in 2i and the right child is in 2i+1 and the parent is in position i/2. The only drawback of this implementation is that the maximum size of the heap needs to be known beforehand. Throughout the rest of the section we will draw heaps as trees but the underlieing implementation will be a list.\n",
    "\n",
    "### Heap-Order Property\n",
    "The property that allows operations to be performed quickly is the **heap-order property**. Since we want to find the minimum as quick as possible it makes sense that the smallest element will always be at the root. If we consider that any subtree also follows this property, then any parent should be smaller than it's children. \n",
    "\n",
    "Applying this logic we arrive at the heap-order property. In a heap, for every node X, the value of X is always smaller than it's children. So by the heap-order property finding the minimum element can always be done in constant time. \n",
    "\n",
    "### Basic Heap Operations\n",
    "For both removing and inserting we need to make sure the heap-order property is maintained \n",
    "\n",
    "#### Insert\n",
    "To insert an element X into the heap, we create a node in the next available location otherwise the tree would no longer be complete. If X can be placed in this node without violating heap order then it is inserted and we are done. Otherwise we slide the node up towards the root, swapping places with the parent node. We continue this process until X can be placed in the correct location. This strategy is known as **percolate up**; the new element is percolated up the heap until the correct location is found. The worst case run time of an insert is O(logN) only is the new element is the new minumum of the heap, i.e. it percolates all the way up to the root. Let's look at an example of inserting 14 into the binary heap below:\n",
    "<img src=\"./files/Heaps/binary-heap-insert-1.png\" width=\"600\"/>\n",
    "<center>We first attempt to insert 14 as the new leaf node but 14 $\\lt$ 31. We then percolate up making 31 the new leaf and re-attempt to place 14.</center>\n",
    "<img src=\"./files/Heaps/binary-heap-insert-2.png\" width=\"600\"/>\n",
    "<center>We attempt to insert 14 again in the new position. Here 14 $\\gt$ 13 so we insert it.</center>\n",
    "\n",
    "#### deleteMin\n",
    "Deleting the minimum element is similar to insertions. When removing the minimum we replace the root with an empty node. Since the heap now becomes one it follows that the last element X in the heap must move somewhere. If X can be placed in the empty node then we are done. This is unlikely because of the heap-order. Instead we swap places with the smaller of the nodes two children, thus pushing the empty node down one level. This means that X is greater than only one of two children. We repeat this until X can be placed in the empty node. Thus our action is to place X in it's correct spot along a path from the root containing minimum children. This strategy is known as **percolate down**. This operation runs in O(logN). Let's look at an example removing 13:\n",
    "<img src=\"./files/Heaps/binary-heap-removal-1.png\" width=\"600\"/>\n",
    "<center>First we remove the root and replace it with an empty node. Next we find the last element in the heap and remove it until we can find where to place it.</center>\n",
    "<img src=\"./files/Heaps/binary-heap-removal-2.png\" width=\"600\"/>\n",
    "<center>We try 31 as the new root but 31 $\\gt$ 14 and 31 $\\gt$ 16. So we take the less of the two children and push it to the parent which is 14. Now we check with the nodes new children. 31 $\\gt$ 21 and 31 $\\gt$ 19. We repeat the process and promote the smaller of the two children, 19.</center>\n",
    "<img src=\"./files/Heaps/binary-heap-removal-3.png\" width=\"600\"/>\n",
    "<center>Next we check against the two children. 31 $\\gt$ 26 so we promote the smaller of the two children, 26. Here we are at a leaf node and insert 31 into it's correct place</center>\n",
    "\n",
    "\n",
    "### Other Heap Operations\n",
    "#### Decrease Key\n",
    "The decrease_key(p, $\\Delta$) operation lowers the value of the item at position p by a positive amount $\\Delta$. If we decrease the amount such that the value of p violates the heap-order we would need to perform a **percolate up** to fix it. This operation could be useful if we need to increase the priority of an item in the heap. \n",
    "\n",
    "#### Increase Key\n",
    "The increase_key(p, $\\Delta$) operation increases the value of the item at position p by a positive amount $\\Delta$. If we incrase the amount such that the value of p violates the heap-order we need to perform a **percolate down** to fix it. This operation allows us to decrease the priority of items in the heap. \n",
    "\n",
    "#### Delete\n",
    "the delete(p) operation removes a specific node from the heap. To achieve this we first perform a decrease_key(p $\\infty$) and then performing a delteMin. An example of when this is needed is we could have a program waiting for resources in the priority queue but it is terminated by the user before reaching the front.\n",
    "\n",
    "#### Build Heap\n",
    "A binary heap is sometimes contructed from an initial list of items. The general algorithm for this is to place all the items into the heap in any order while maintaining the strucutre of a complete binary tree. Afterwards we perform a percolateDown(p) on the parent of each subtree to enforce the heap-order property of the heap. This operation runs in O(N) time. \n",
    "\n",
    "## Applications of binary heaps\n",
    "### The selection problem\n",
    "The selection problem is that given an input list of N elements find the $k^{th}$ largest element. Initially without the use of a heap we could perform this by first sorting the list and return the $k^{th}$ integer. Here if we assume a simple sorting algorithm it would take O(N^2). With heaps this could be achieved in on average O(n). To solve this using heaps we first build a heap of the N elements which takes O(N) time then perform k deleteMin().  Worst case is that k = 1 and then it will run in O(NlogN). To avoid this we could simply use a maximal heap instead that way only k deleteMin() instead of N-k will have to performed to find the kth largest element. \n",
    "\n",
    "Now let's look at the code for a binomial heap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf\n",
    "\n",
    "class MinHeap(object):\n",
    "    def __init__(self):\n",
    "        # for use to use i/2 as the parent we must start at index 1 as the first element\n",
    "        self.heap = [0]\n",
    "        self.size = 0\n",
    "        \n",
    "    # performed for inserts\n",
    "    def _percolate_up(self, i):\n",
    "        while i // 2 > 0: # we use // because it forces integer division\n",
    "            if self.heap[i] < self.heap[i // 2]: # check if the child is less than the parent\n",
    "                # swap the values\n",
    "                tmp = self.heap[i // 2]\n",
    "                self.heap[i // 2] = self.heap[i]\n",
    "                self.heap[i] = temp\n",
    "            i = i // 2\n",
    "            \n",
    "    def _percolate_down(self, i):\n",
    "        while i * 2 < self.size:\n",
    "            # get which child is the smallest\n",
    "            mc = self._min_child(i)\n",
    "            if self.heap[mc] < self.heap[i]:\n",
    "                temp = self.heap[i]\n",
    "                self.heap[i] = self.heap[mc]\n",
    "                self.heap[mc] = temp\n",
    "                i = mc\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    def _min_child(self, i):\n",
    "        # first check if the parent has a right child (since we insert from left to right)\n",
    "        if 2*i + 1 > self.size:\n",
    "            return 2*i\n",
    "        # Otherwise check both children\n",
    "        elif self.heap[2*i] < self.heap[2*i+1]:\n",
    "            return 2*i\n",
    "        else:\n",
    "            return 2*i+1\n",
    "    \n",
    "    def insert(self, k):\n",
    "        self.heap.append(k)\n",
    "        self.size += 1\n",
    "        self._percolate_up(self.size)\n",
    "        \n",
    "    def delete_min(self):\n",
    "        min_val = self.heap[1]\n",
    "        self.heap[1] = self.heap[self.size]\n",
    "        self.size -= 1\n",
    "        self.heap.pop() # this removes the last element in the list\n",
    "        self._percolate_down(1)\n",
    "        return min_val\n",
    "    \n",
    "    def decrease_key(self, i, k):\n",
    "        # bounds check\n",
    "        if 1 <= i and i <= self.size:\n",
    "            self.heap[i] -= k\n",
    "            # check and see if heap-order has been violated\n",
    "            if self.heap[i // 2] > self.heap[i]:\n",
    "                self._percolate_up(i)\n",
    "                \n",
    "    def increase_key(self, i, k):\n",
    "        # bounds check\n",
    "        if 1 <= i and i <= self.size:\n",
    "            self.heap[i] += k\n",
    "            # check if it violates the heap-order\n",
    "            # first check if it has children\n",
    "            if i * 2 < self.size:\n",
    "                # here we have to check both children\n",
    "                if self.heap[i] > self.heap[2*i] or self.heap[i] > self.heap[2*i+1]:\n",
    "                    self._percolate_down(i)\n",
    "            elif i * 2 == self.size:\n",
    "                # check only the one child\n",
    "                if self.heap[i] > self.heap[2*i]:\n",
    "                    self._percolate_down(i)\n",
    "    \n",
    "    def delete(self, i):\n",
    "        self.decrease_key(i, inf)\n",
    "        self.delete_min()\n",
    "        \n",
    "    def build_heap(self, init_list):\n",
    "        self.heap = [0] + init_list\n",
    "        self.size = len(init_list)\n",
    "        i = self.size // 2\n",
    "        while (i > 0):\n",
    "            self._percolate_down(i)\n",
    "            i -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 26, 31, 58, 41, 53, 59, 97]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heap = MinHeap()\n",
    "heap.build_heap([31, 41, 59, 26, 53, 58, 97])\n",
    "heap.heap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *d*-Heaps\n",
    "Similar to trees we can generalize binary heaps into a **d-heap**, which is exactly like a binary heap except each node has d children. A d-heap is shallower than an equivalent binary heap, improving the running time of inserts to $O(log_dN)$. However, for large d, the deleteMin operation is more expensive, because even though the tree is shallower, the minimum of d children must be found, which takes d-1 comparisons. This raises the time of the operation to $O(d*log_dN)$. Since d is a constant we can generalize both running times to O(logN). With d-heaps we can still use arrays as the implementation however unless d is a power of 2, it slightly increases the running time, because we can no longer implement division by bit shift. <br>\n",
    "The biggest weakness of heaps, aside from the inability to perform searches, is that combining two heaps into one is a hard operation. This extra operation is known as **merge**. There are quite a few ways of implementing heaps so that the running time of merge is O(logN). The following three types of heaps efficiently support the merge function.\n",
    "\n",
    "\n",
    "## Leftist Heaps\n",
    "Like a binary heap, a **leftist heap** has both a structural and ordering property. It has the same heap-order property however and is also a binary tree. The only difference between a leftist heap and a binary heap is that leftist heaps are not perfectly balanced, instead they attempt to be unbalanced. \n",
    "\n",
    "### Leftist Heap Property\n",
    "We define the **null path length**, *npl(X)*, of any node X to be the length of the shortest path from X to a node without two children. This, the *npl* of a node with zero or one child is 0.<br>\n",
    "The leftist heap property is that for every node X in the heap, the null path length of the left child is at least as large of the right child. As we can see in the image below. Only the left tree is a leftist heap. This is because in the right tree the property is violated when the null path of the left child is 0 and the right child is 1. \n",
    "<img src=\"./files/Heaps/leftist-heap.png\" width=\"600\"/>\n",
    "\n",
    "This property forces the tree to be unbalanced, because it clearly biases the tree to go deep toward the left, hence the name leftist heap. The general idea of a leftist heap to perform all the operations on the right path, which is guranteed to be shorter than the left. \n",
    "\n",
    "### Leftist Heap Operations \n",
    "The fundamental operation on leftist heaps is merging. In this case we can view insertion as a merge of a one-node heap. The solution to merging can be done recursively. Recursively we would work with two heaps h1 and h2. We compare the root of the two heaps and merge the right subheap of the heap with the smaller root into the heap with the larger root. We recursively do this until we reach an empty node. While this does result in a heap that maintains heap-order it is no longer a leftist heap. We can easily fix this by checking the result of each merge. After each merge we check the npl values of the children, if the npl value of the right child is greater than that of the left we switch the children to maintain the leftist heap property. <br>\n",
    "Let's look at the code to implement this:\n",
    "```python\n",
    "def merge(h1, h2):\n",
    "    # Base case\n",
    "    if h1 == None:\n",
    "        return h2\n",
    "    if h2 == None:\n",
    "        return h1\n",
    "    else:\n",
    "        # Check which root has the smaller value\n",
    "        if h1.value < h2.value:\n",
    "            h1.right = merge(h1.right, h2)\n",
    "            if h1.left.npl < h1.right.npl:\n",
    "                swap_children(h1)\n",
    "            h1.npl = h1.right.npl + 1\n",
    "            return h1\n",
    "        else:\n",
    "            h2.right = merge(h2.right, h1)\n",
    "            if h2.left.npl < h2.right.npl:\n",
    "                swap_children(h2)\n",
    "            h2.npl = h2.right.npl + 1\n",
    "            return h2\n",
    "```\n",
    "Since we are only merging along the right sub heap each time the merge algorithm runs in O(logN). Before as we said with insert we can simply treat this as a merge of the original heap plus a one node heap. For deletion we would merge the right and left subtrees of the root to create the new leftist heap. \n",
    "\n",
    "Lets look at an example of merging two leftist heaps togther. Lets start with two heaps $H_1$ and $H_2$\n",
    "<img src=\"./files/Heaps/leftist-heap-1.png\" width=\"600\"/>\n",
    "We initially call merge(h1,h2) which results in comparing the root 3 and 6. Since 3 < 6 we merge the right subtree of $H_1$ with $H_2$. This results in comparing 6 and 8. Since 6 < 8 we merge the right subtree of $H_2$ with $H_1$'s right subtree rooted at 8. We continue with the following comparisons (7,8), (18,8) and finally (18, null). We've hit a null node so we make 18 the new right child of 8. \n",
    "<img src=\"./files/Heaps/leftist-heap-2.png\" width=\"250\"/>\n",
    "\n",
    "Now we back up our recursive calls and merge 8 with 7. This results in a tree that is not a leftist heap so we swap the two children to solve this. \n",
    "\n",
    "<img src=\"./files/Heaps/leftist-heap-3.png\" width=\"500\"/>\n",
    "\n",
    "Then we merge the result with 6. This does not violate the heap property as it's still a leftist heap. So we finally merge back with 3 and notice the heap violates the leftist heap property and swap the children resulting in the final leftist heap.\n",
    "\n",
    "<img src=\"./files/Heaps/leftist-heap-4.png\" width=\"600\"/>\n",
    "\n",
    "## Skew Heaps\n",
    "A **skew heap** is a self-adjusting version of a leftist heap that is incredibly simple to implement. The relationship of skew heaps to leftist list is analogous to the relation between splay and AVL trees. Skew heaps are binary heaps with heap order, but there is no structural constraint on these trees. Unlike leftist heaps, no information is maintained about the null path length of any node. Similar to leftist heaps, skew heap operations also run in O(logN). As with left heaps the main operation of skew heaps is merging. The merge routine is again recursive, and the exact same operations as leftist heaps are performed with one exception. The main difference between skew heaps and leftist heaps is that in leftist heaps we check to see whether the left and right children satisfy the leftist heap structure property. For skew heaps the swap is unconditional and it is *always* performed.\n",
    "\n",
    "## Binomial Queues\n",
    "Although both leftist and skew heaps support merging, insert, and deleteMin that all run in O(logN) time, there is room for improvement. **Binomial Queues** improve upon the insertion operation to run in constant O(1) time on average, but worst case is still O(logN), while deleteMin and merge remain O(logN)\n",
    "\n",
    "### Binomial Queue Structure\n",
    "Binomial Queues differ from the other priority queue implementations as a binomial queue is not a heap ordered tree. It is actually a collection of heap-ordered trees, known as a **forest**. Each of the heap-ordered trees is a **binomial tree**. There is at most one binomial tree of every height. A binomial tree of height 1 is a one-node tree. A binomial tree, $B_k$ of height k-1 is formed by attaching a binomial tree, $B_{k-1}$, to the root of another binomial tree, $B_{k-1}$. The following shows binomial tree $B_0,\\, B_1,\\, B_2,\\, B_3,\\, and\\, B_4$\n",
    "<img src=\"./files/Heaps/binomial-tree.png\" width=\"600\"/>\n",
    "\n",
    "As we can see a binomial tree, $B_k$, is made up of a root with children $B_0,\\, B_1,...,\\, B_{k-1}$. If we impose heap order on the binomial trees and allow at most one binomial tree of any height, we can represent a priority queue of any size by a collection of binomial trees. For instance a priority queue of size 13 could be represented by the forest $B_3,\\, B_2,\\, B_0$. We can even represent this in binary as 1101, which is 13 in binary, showing that $B_0$ is missing from the forest. Here we see an example of a binomial queue of six elements made up of $B_2,\\, B_1$\n",
    "<img src=\"./files/Heaps/binomial-queue-6.png\" width=\"275\"/>\n",
    "\n",
    "### Binomial Queue Operations\n",
    "The minimum element can be found by scanning the roots of of all the trees in the queue. Since there are at most logN different trees, the minimum can be found in O(logN) time. <br>\n",
    "Merging two binomial queues is quite simple as we will see in an example. Let's look at two queues $H_1$ and $H_2$. \n",
    "<img src=\"./files/Heaps/binomial-queue-merge-1.png\" width=\"350\"/>\n",
    "The merge is formed by basically added the two queues together. We will create a new binomial queue $H_3$ that will be the new binomial queue that is the merge of $H_1$ and $H_2$. Since $H_1$ has no binomial queue of height 0 and $H_2$ does, we will just use the binomial tree of $H_2$ as part of $H_3$. Next we add binomial trees of height 1. Since both $H_1$ and $H_2$ both have binomial trees of height 1, we merge them together by making the tree with the larger root the subtree of tree with the smaller root. This creates a new tree of height 2. As we can see here:\n",
    "<img src=\"./files/Heaps/binomial-queue-merge-2.png\" width=\"500\"/>\n",
    "Thus $H_3$ will not have a binomial tree of height 1. There are now three binomial trees of height 2, the two from $H_1$ and $H_2$ plus the one we just created from the merge. We keep one binomial tree of height 2 and merge the other two. Which of the two you merge does not matter in this case we will merge the newly created tree and the one from $H_1$ creating a binomial tree of height 3, which becomes part of $H_3$. Since we did not merge the tree of height 2 from $H_2$ it becomes the tree of height two in $H_3$. Here we can see the resulting queue of the two binomial queues merged.\n",
    "<img src=\"./files/Heaps/binomial-queue-merge-3.png\" width=\"600\"/>\n",
    "Since merging two trees takes constant time and there are logN trees, the merge takes O(logN). To make this efficient we need to keep the trees in the queue sorted by height which is simple to do if we use an array where each index holds the tree correlating to that height.\n",
    "\n",
    "Insertion is just a special case of merging, similar to skew and leftist heaps. We create a binomial tree of height 0 and merge it into the queue. To prove that insertion on average is constant time lets analyze the running time of performing N inserts into an initially empty binomial queue. This will take O(N) worst-case time. \n",
    "\n",
    "###### Claim\n",
    "A binomial queue of N elements can be built by N successive insertions in O(N) time.\n",
    "\n",
    "Let us consider the result of an insertion. If there is no $B_0$ tree present at the time of the insertion, then the insertion total cost is one unit. The result is there is now a $B_0$ tree, and thus we have added a tree to the forest. If there is a $B_0$ tree but no $B_1$ tree, then the insertion cost is two units. The new forest will have a $B_1$ tree but no longer have a $B_0$ tree, so the number of tress in the forest is unchanged. An insertion that costs three units will create a $B_2$ tree but destroy a $B_0$ and $B_1$ tree, yielding a net loss of one tree in the forest. Let us generalize the insertion costs $c$ units results in a net increase of $2-c$ trees in the forest, because $B_{c-1}$ tree is created but all $B_i$ tree $\\le i \\lt c - 1$ are removed. Thus expensive insertions remove trees, while cheap insertions create trees. <br>\n",
    "Let $C_i$ be the cost of the ith insertion. Let $T_i$ be the number of tress *after* the ith insertion. $T_0$ = 0 is the number of trees initially (as the queue is empty). Then we have the invariant: <br><br>\n",
    "\\begin{align}\n",
    "C_i + (T_i - T_{i-1}) = 2 \\\\ \n",
    "\\end{align}\n",
    "We then have <br><br>\n",
    "\\begin{align}\n",
    "C_1 + (T_1 - T_0) = 2 \\\\\n",
    "C_2 + (T_2 - T_1) = 2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "C_{N-1} + (T_{N-1} - T_{N-2}) = 2 \\\\\n",
    "C_N + (T_{N} - T_{N-1}) = 2\n",
    "\\end{align}\n",
    "If we were to add all of these equations up in to a summation, most of the $T_i$ terms cancel out, leaving <br><br>\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{N} C_i + T_N - T_0 = 2N\n",
    "\\end{align}\n",
    "or equivaliently <br><br>\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{N} C_i = 2N - (T_N - T_0)\n",
    "\\end{align}\n",
    "Recall that $T_0 = 0$ and $T_N$, the number of trees after N insertions, is certainly not negative, so $(T_N - T_0)$ is not negative. Thus <br><br>\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{N} C_i \\le 2N\n",
    "\\end{align}\n",
    "which proves the claim. Since we have proved the claim that performing N successive insertions only takes O(N) time then we can assume that on average each insertion ran in O(1) time. <br>\n",
    "A deleteMin can be performed by first finding the binomial tree with the smallest root. Let this tree be $B_k$, and let the original priority queue be H. We remove the binomial tree $B_k$ from the forest of trees in H, forming a new binomial queue $H'$. We also remove the root of $B_k$ creating binomial trees $B_0,\\, B_1,\\, ...,\\, B_{k-1}$, which collectively form priority queue $H''$. We finish the operation by merging $H`$ and $H``$. For an example lets look at performing a deleteMin on $H_3$.\n",
    "<img src=\"./files/Heaps/binomial-queue-remove-1.png\" width=\"600\"/>\n",
    "The minimum root is 12, so we obtain two binomial queues $H'$ and $H''$. Where $H'$ is the result of removing $B_3$ from $H_3$ and $H''$ is the result of removing 12 from $B_3$.\n",
    "<img src=\"./files/Heaps/binomial-queue-remove-2.png\" width=\"350\"/>\n",
    "<img src=\"./files/Heaps/binomial-queue-remove-3.png\" width=\"350\"/>\n",
    "We then perform a merge on $H'$ and $H''$ resulting in the tree below with 12 removed\n",
    "<img src=\"./files/Heaps/binomial-queue-remove-4.png\" width=\"500\"/>\n",
    "For run time analysis note first that the deleteMin operation breaks the original binomial queue into two. It takes O(logN) time, individually, to find the binomial tree with the minium root and to create the queues $H'$ and $H''$. So together these operations take O(2logN). Merging the two queues takes O(logN) time, so the entire deleteMin operation takes O(3logN) $\\rightarrow$ O(logN) time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
